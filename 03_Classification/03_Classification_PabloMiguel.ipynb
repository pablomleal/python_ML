{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main objective of the analysis\n",
    "\n",
    "The purpose of the analysis is to solve an engineering problem. \n",
    "Traditional smoke-detector devices are based on the opacity of the air: when it contains smoke, the light is not allowed through the sensor and then the fire alarm is rung.\n",
    "These devices use high frequency range radionucleides, with its associated cost.\n",
    "\n",
    "In this analysis we want to question if an alternative, cheaper device can be built based on metrics related to the chemical contents of the air.\n",
    "Therefore, and as the purpose is of the engineering type, the purpose is rather predictive than interpretative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('smoke_detection_iot.csv')\n",
    "data.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do a first look up of my data.\n",
    "I can see that all of the data are numeric, i.e. either int or float, in a free range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this result, we can conclude that there are two columns (Observation and time -UTC- which will bring little to the analysis and will add preocessing time, so we will delete them eventually).\n",
    "Moreover, the dataset is homogeneous in shape: all features are numeric quantities, so no label encoding is needed.\n",
    "\n",
    "Now I'd like to have a first idea of the correlation between variables. For this I'll use the corr method from pandas and plot it in a color map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt,numpy as np\n",
    "plt.title(\"Correlation between features\")\n",
    "plt.imshow(data.corr(), interpolation='nearest')\n",
    "\n",
    "eje = np.arange(len(data.columns.tolist()))\n",
    "plt.colorbar()\n",
    "plt.xticks(eje, data.columns,rotation=90)\n",
    "plt.yticks(eje, data.columns,rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In view of the above we can immediately notice that some of the features (PM1.0, PM2.5, etc) are correlated with one another, which may point at potential colinearity among them.\n",
    "\n",
    "As per the outcome we have selected, correlations seem to be rather and mixed in general as there is no single feature capable of accounting, alone, for the existence of fire. We will see if this yields any result in the next points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to make sure that Fire Alarm is of type boolean, that is it contains only two values representing True or False.\n",
    "Indeed, the Fire alarm seems to be boolean, but there is a clear predominance of 1's, so a stratified shuffle split may be interesting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fire Alarm'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be dropping the first column (observation number) which is useless. \n",
    "Also pick up the Fire Alarm as the outcome (ground truth), whereas the rest of the columns will be features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the observation column, selecting outcomes and features.\n",
    "data.drop(columns=['Observation', 'UTC'], inplace=True)\n",
    "outcome = data['Fire Alarm']\n",
    "featuresColumns = [x for x in data.columns if x!=\"Fire Alarm\"]\n",
    "features = pd.DataFrame(data[featuresColumns])\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In view of the above, we need to make a stratified test split so that we can weigh in the different classes proportionately. The output shows that, in effect, the samples are proportionate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msilib.schema import FeatureComponents\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "myShuffle = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "#StratifiedShuffleSplit()\n",
    "#a = myShuffle.split(features,outcome)\n",
    "\n",
    "train_idx, test_idx = next(myShuffle.split(features, outcome))\n",
    "\n",
    "X_train = features.loc[train_idx, featuresColumns]\n",
    "y_train = outcome.loc[train_idx]\n",
    "\n",
    "X_test = features.loc[test_idx, featuresColumns]\n",
    "y_test = outcome.loc[test_idx]\n",
    "\n",
    "print(y_train.value_counts(normalize=True), y_test.value_counts(normalize=True), outcome.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of three different classifier models\n",
    "\n",
    "I will be using the following three classifier models:\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "- Ensemble model based on Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all three models, first we will make this function in order to be able to produce the performance metrics of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def measure_error(y_true, y_pred, label):\n",
    "    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n",
    "                      'precision': precision_score(y_true, y_pred),\n",
    "                      'recall': recall_score(y_true, y_pred),\n",
    "                      'f1': f1_score(y_true, y_pred)},\n",
    "                      name=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "lr_Metrics = measure_error(y_test,y_pred_lr, 'Logistic regression')\n",
    "lr_Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made the attempt of checking the confusion matrix and plotting it, however, as there are only 2 classes it doesn't provide a lot of information. I will be using the metrics (recall, precision) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "\n",
    "eje = [0,1]\n",
    "plt.colorbar()\n",
    "plt.xticks(eje, eje)\n",
    "plt.yticks(eje, eje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "The procedure is exactly the same as with Logistic regression, but with the right class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "dt_Metrics = measure_error(y_test, y_pred_dt, 'Decision Trees')\n",
    "dt_Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "In this case we need to make a first pre analysis to see what an appropriate number of estimators could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "RF = RandomForestClassifier(oob_score=True, \n",
    "                            random_state=42, \n",
    "                            warm_start=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "oob_list = list()\n",
    "\n",
    "for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n",
    "    \n",
    "    RF.set_params(n_estimators=n_trees)\n",
    "    RF.fit(X_train, y_train)\n",
    "    oob_error = 1 - RF.oob_score_    \n",
    "    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n",
    "\n",
    "rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "rf_oob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model converges to a lower error very, very fast. In principle 40 estimators should serve the purpose.\n",
    "However if I put only 5 estimators, here is what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFObject_final = RandomForestClassifier(oob_score=True, \n",
    "                            random_state=42, \n",
    "                            warm_start=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "rf_final = RFObject_final.set_params(n_estimators=5)\n",
    "\n",
    "rf_final.fit(X_train, y_train)\n",
    "y_pred_RF = rf_final.predict(X_test)\n",
    "\n",
    "\n",
    "rf_metrics = measure_error(y_test, y_pred_RF, 'Random Forest')\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we achieve a perfect classification with just 5 estimators.\n",
    "Just to make sure that this is not an eror, I am repeating the experiment with just 2 estimators and this was the result:\n",
    "\n",
    "| Accuracy   |      Precision      |  Recall | F1|\n",
    "|------------|:-------------------:|---------|---|\n",
    "|  0.999521  |  0.999925           | 0.999404|0.999665|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to plot the series for our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFigure = plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.plot(lr_Metrics, label=\"Logistic Regression\")\n",
    "plt.plot(dt_Metrics, label=\"Decision Tree\")\n",
    "plt.plot(rf_metrics, label=\"Random Forest\")\n",
    "ax = plt.axis(ymin=0.985, ymax=1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which of your classifier models would you recommend as a final model that best fits your needs in terms of accuracy and explainability?\n",
    "\n",
    "Overall, the reliability of all 3 models was very high. Note that, in the chart above, the base scale is .985, which is already very high.\n",
    "\n",
    "However, based on the experiments, the Random Forest was the one that provided the best results in quality as, we have achieved a perfect classifier (no false positives or negatives).\n",
    "\n",
    "\n",
    "Considering that the objective is setting the base to build a critical security device, I believe the reliability is important, especially if it's achievable at a reasonable efficiency (as only 5 decision trees where needed).\n",
    "\n",
    "\n",
    "##### Summary Key Findings and Insight\n",
    "The key finding is that the characterization of the fire can be done with high degree of confidence.\n",
    "Therefore the next question would be whether that the profit of building device with sensors for all features can outweigh that of traditional fire sensors.\n",
    "\n",
    "Also, in a more personal level, I found out that once we have made our split, the models are quite exchangeable, which allows for an easier utilization in ensemble methods.\n",
    "\n",
    "\n",
    "##### Suggestions for next steps in analyzing this data\n",
    "\n",
    "Taking into account the original objective of this analysis, the next step would be to ensure a compromise between the data scientist and the manufacturing team.\n",
    "\n",
    "We have made a good model, but at the cost of installing up to 14 sensors in the device. After all, it's hardly imaginable that a device based on 14 sensors may be competitive against the well-known current devices.\n",
    "\n",
    "In order to do this, however, we will need to have an interpretative analysis. This I had discarded this approach because the more urgent, mandatory question was whether a classification is achievable. \n",
    "After confirming that it is, it would be time to see the real role that each of these 14 features play in the prediction of fire (for which a transformation and other steps would be needed).\n",
    "\n",
    "I would engage in conversations with the team in charge of manufacturing the device to know more about the manufacturing contraints. For example:\n",
    "- Which sensors devices that can be built at a reasonably low cost\n",
    "- Which can still be built but at a higher cost,\n",
    "- Which cannot be built, \n",
    "\n",
    "And then feedback those constraints to my model to see if we can achieve a compromise between data science and manufacturing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a642a9a94d05d387b7a27be99b5a4ff6656c8c24931cc81c2f78dd14cfb5dc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
