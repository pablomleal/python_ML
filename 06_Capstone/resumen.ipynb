{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMANA 3:\n",
    "\n",
    "### Content-based recommendation (CoBR) based on user profile.\n",
    "\n",
    "User profile: matriz en la que están listadas las preferencias User -> Tags.\n",
    "Se puede construir a partir del producto de dos matrices:\n",
    "- El usuario pone Ratings sobre cada Item: User -> Items \n",
    "- Cada Item tiene unos tags o features: Items -> Tags.\n",
    "- Esto nos produce la matriz de perfiles de usuario User -> Tags.\n",
    "\n",
    "La idea es que si multiplicamos un vector de features de un nuevo Item (Items -> Tags) por un vector de perfil de usuario (User -> Tags) obtendremos un score.\n",
    "\n",
    "El ejercicio ha consistido en:\n",
    "- Para un pool de usuarios de prueba, sacar un lista de los cursos que NO han visitado aún.\n",
    "- Para cada usuario, y para cada curso NO visitado, sacar un Score basándose en su perfil (características del curso multiplicado por perfil).\n",
    "- Si en cada caso el score supera un umbral, se añade a un listado.\n",
    "\n",
    "### Content-based recommendation (CoBR) based on course similarity.\n",
    "\n",
    "Este capítulo presupone la existencia de una matriz triangular de similitud entre los items, precalculada de alguna forma.\n",
    "Lo único que hay que hacer en este caso es acceder a la matriz de similitud por los índices correctos. Para ello hay que hacer previamente una función auxiliar que permita averiguar primero esos índices.\n",
    "\n",
    "\n",
    "Contando con eso, lo único que hacemos es visitar dicha matriz por las filas de los cursos ya visitados y extramos los valores de aquellas columnas no coincidentes con los mismos que excedan un umbral. Por ejemplo:\n",
    "\n",
    "Hay un total de 5 cursos y el usuario ya ha visitado los cursos 2º y 3º. El proceso sería:\n",
    "- Entrar en la matriz por la fila 2.\n",
    "-- Comprobar la similitud con el curso 1, es decir, el elemento (2, 1). Si es mayor del umbral, lo apuntamos.\n",
    "-- El elemento (2, 2) nos lo saltamos, ya que sería analizar un elemento ya visitado por el usuario, y además, contra sí mismo.\n",
    "-- El elemento (2, 3) nos lo saltamos porque también es un curso ya visitado.\n",
    "-- Comprobamos similitud de componentes (2, 4) y (2, 5).\n",
    "\n",
    "- Repetiremos el proceso con la fila 3 (elementos  (3,1), (3,4) y (3,5) ).\n",
    "\n",
    "### Clustering (Base para Collaborative Recommendation Systems).\n",
    "\n",
    "En este caso volvemos a utilizar el vector de perfiles de usuario. Sin embargo, en lugar de lanzar mi vector contra una nueva película, lo que vamos a hacer es ver qué han consumido personas con vectores similares a los míos, es decir, hay que clusterizar la matriz de perfiles de usuario.\n",
    "\n",
    "Hecho eso, sólo hay que mirar qué películas han votado positivamente la gente de mi cluster en concreto. Esto nos lleva, de nuevo, a regresar a los vectores de ratings.\n",
    "\n",
    "Este ejercicio no tiene nada que ver con los anteriores, ya que procura separar en clusters la matriz de user profiles.\n",
    "Es decir, lo que estamos intentando hacer es agrupar usuarios con gustos similares.\n",
    "\n",
    "A continuación se hace un ejercicio de PCA que lo que nos dice es que podríamos utilizar una matriz más sencilla para optimizar el clustering obteniendo casi los mismos resultados.\n",
    "\n",
    "## SEMANA 4:\n",
    "\n",
    "### Collaborative filtering using KNN.\n",
    "\n",
    "En este caso el objetivo es el mismo pero se trata de hacerlo SIN disponer de la matriz de perfiles de usuario.\n",
    "Ahora sabemos que a esto se le llamaba filtrado colaborativo basado en el usuario.\n",
    "Pero hay otro que está centrado en los items.\n",
    "\n",
    "Si lo entiendo bien, se trata de, en lugar de utilizar la matriz de perfiles (user-tags), emplear más bien la de user-items.\n",
    "\n",
    "\n",
    "Dadas las dificultades inherentes al método KNN (uso intensivo de memoria) este ejercicio me ha obligado a trabajar mucho con la gestión de dataframes, incluyendo las opciones de acceso a los mismos (masks, loc, iloc); la forma de realizar matrices dispersas a patir de matrices densas, a través de un uso sencillo del método pivot, y la facilidad con que se puede llegar a utilizar métodos de dataframe como groupby.\n",
    "\n",
    "He aprendido a valorar si es útil mantener un índice o no, ya que me suelo manejar mejor descartando los llamados índices de rango.\n",
    "\n",
    "En el ejercicio he realizado los siguientes pasos.\n",
    "\n",
    "- Elaboración manual de una matriz de similitud a partir de la matriz de interacciones de usuario.\n",
    "- Luego, a diferencia del ejercicio del módulo 3 (encontrar cursos similares a los que yo he cursado), se ha realizado un cluster de usuarios para agrupar usuarios que puntúan similarmente. Es decir, el foco no estaría en la similitud de contenido sino en preferencias de usuarios vecinos. Para ello, el procedimiento es:\n",
    "- Construir la matriz de similitud;\n",
    "- Para cada usuario que proporciona un rating a un curso:\n",
    "--- encontrar sus K vecinos próximos según la matriz de similtud;\n",
    "--- Para dichos vecinos, averiguar (desde la matriz de interacciones) los ratings que han dado a ese mismo curso;\n",
    "--- Calcular una media ponderada de esos ratings vs. similitudes para predecir un nuevo rating,\n",
    "- Esto iterado sobre todos los ratings nos dará una predicción que en mi caso no ha superado el 55%. No es una predicción muy buena considerando que la predicción con el módulo Surprise estaba por encima del 95%. \n",
    "\n",
    "Además, subiendo el dataset a 1000 elementos esta precisión ha descendido otro poco más, así como aumentando el número de vecinos.\n",
    "\n",
    "\n",
    "\n",
    "Una reflexión, y es que puesto que he entendido que en KNN, el modelo es el propio dataset, por lo que no tengo claro si es necesario pensar en train y test datasets. Por lo tanto he utilizado todos los data points para mis comprobaciones. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
